---
title: "Predictive Modeling of COVID-19"
author: "Aziz Asomiddinov, Tom Youngblood"
date: "August 21, 2024"
output: pdf_document
---

# Introduction

## Abstract

This R-Markdown file contains an exploratory data analysis of Kaggle's COVID-19 Dataset, and the process of fitting two machine learning models, LDA and QDA, to determine the variables associated with death by COVID-19. 

There are five sections:

1. Data Wrangling

2. Exploratory Data Analysis

3. Variable Selection

4. Predictive Modeling

5. Conclusions

## Data

The dataset, Kaggle's 'COVID-19 Dataset', can be found at this link: https://www.kaggle.com/datasets/meirnizri/covid19-dataset.

The content of the dataset contains 21 features and 1,048,576 unique patients. The features are listed below (source: Kaggle user meirnizri):

- sex: 1 for female and 2 for male.

- age: of the patient.

- classification: covid test findings. Values 1-3 mean that the patient was diagnosed with covid in different.

- degrees. 4 or higher means that the patient is not a carrier of covid or that the test is inconclusive.

- patient type: type of care the patient received in the unit. 1 for returned home and 2 for hospitalization.
- pneumonia: whether the patient already have air sacs inflammation or not.

- pregnancy: whether the patient is pregnant or not.

- diabetes: whether the patient has diabetes or not.

- copd: Indicates whether the patient has Chronic obstructive pulmonary disease or not.

- asthma: whether the patient has asthma or not.

- inmsupr: whether the patient is immunosuppressed or not.

- hypertension: whether the patient has hypertension or not.

- cardiovascular: whether the patient has heart or blood vessels related disease.

- renal chronic: whether the patient has chronic renal disease or not.

- other disease: whether the patient has other disease or not.

- obesity: whether the patient is obese or not.

- tobacco: whether the patient is a tobacco user.

- usmr: Indicates whether the patient treated medical units of the first, second or third level.

- medical unit: type of institution of the National Health System that provided the care.

- intubed: whether the patient was connected to the ventilator.

- icu: Indicates whether the patient had been admitted to an Intensive Care Unit.

- date died: If the patient died indicate the date of death, and 9999-99-99 otherwise.

# Data Wrangling
The data is loaded and cleaned in this section.

## Loading Data

Libraries must be loaded first.
```{r Libraries}
library(ggplot2)
library(dplyr)
library(tidyverse)
library(tidyr)
library(MASS)
library(ggpubr)
library(caret)
library(car)
library(pROC)
library(leaps)
```

The data is loaded in the cell below.
```{r Loading Data: Initial Findings}
data <- read_csv('Covid Data.csv')

head(data)
```

### Data Cleaning: Binary Response Variable Creation

As seen in the table above, there is no binary response variable for the intended outcome variable, death. In the code below, the binary variable, 'DEATH', is created.

```{r Data Cleaning: Binary Variables Creation}
# If DATE_DIED not 9999-99-99, patient did not die
data$DIED <- ifelse(data$DATE_DIED != '9999-99-99', 1, 0)
data$DIED <- as.factor(data$DIED)
```

### Data Cleaning: Removal of Missing Values
Values of 97, 98, and 99 indicate missing data; these values are immediately visible in the table above. Missing data values are removed in the cell below.

```{r Data Cleaning: Removal of Missing Values}
# Drop date died
data <- data[,!names(data) %in% c("DATE_DIED", "USMER")]

# Rows before missing value removal
original_nrows <- nrow(data)
cat('Number of rows before missing data removal: ', original_nrows, '\n')

# Missing Value Removal
data <- data <- data[!apply(data, 1, function(row) any(row %in% c(97, 98, 99))), ]

# Drop variables that are problematic later in the analysis
data <- data[,!names(data) %in% c("SEX", "PATIENT_TYPE", "CLASSIFICATION_FINAL")]

# Rows after missing value removal
new_nrows <- nrow(data)
cat('Number of rows after data removal: ', new_nrows, '\n')

# Rows removed
removed <- original_nrows - new_nrows
cat('Number of observations removed: ', removed, '\n')
```

### Data Cleaning: Datatype Conversion

Many of the categorical variables in the data-set are classified as type <dbl>. The LDA and QDA algorithms from the MASS library may treat these variables as ordinal, which they are not. The code-cell below converts the necessary variables to <factor>.

All variables besides those listed below were converted to factor:

- age: of the patient.

- date died: If the patient died indicate the date of death, and 9999-99-99 otherwise.

- usmr: Indicates whether the patient treated medical units of the first, second or third level.

  * usmr was not converted to factor as its data is not useful in factor form, according to R output.
```{r Cleaning Data: Datatype Conversion}
# Convert necessary variables to factor
data$PNEUMONIA <- as.factor(data$PNEUMONIA)
data$PREGNANT <- as.factor(data$PREGNANT)
data$DIABETES <- as.factor(data$DIABETES)
data$COPD <- as.factor(data$COPD)
data$ASTHMA <- as.factor(data$ASTHMA)
data$INMSUPR <- as.factor(data$INMSUPR)
data$HIPERTENSION <- as.factor(data$HIPERTENSION)
data$CARDIOVASCULAR <- as.factor(data$CARDIOVASCULAR)
data$RENAL_CHRONIC <- as.factor(data$RENAL_CHRONIC)
data$OTHER_DISEASE <- as.factor(data$OTHER_DISEASE)
data$OBESITY <- as.factor(data$OBESITY)
data$TOBACCO <- as.factor(data$TOBACCO)
#data$USMER < as.factor(data$USMER)
data$MEDICAL_UNIT <- as.factor(data$MEDICAL_UNIT)
data$INTUBED <- as.factor(data$INTUBED)
data$ICU <- as.factor(data$ICU)

str(data)
```

# Background: LDA and QDA

**Linear Discriminant Analysis (LDA)** and **Quadratic Discriminant Analysis (QDA)** are both classification tasks that predict the probability of a categorical outcome variable belonging to a specific class.

A class is made up of the results of each categories; for example, the categorical variable DIED has two classes:

- Class 1: (DIED = 1)

- Class 2: (DIED = 0)

**Connection between LDA, QDA, and Logistic Regression**: All three methods, LDA, QDA, and Logistic Regression, attempt to predict the probability of a categorical outcome variable based on a set of input variables. The primary difference between the three forms of regression lie in their assumptions:

- Logistic Regression: Does not have any distributional assumptions, but requires a categorical outcome variable.

- LDA: Assumes that the predictor variables are normally distributed, that the predictor variables share the same covariance matrix, and that the outcome variable is categorical. This produces a linear decision boundary.

- QDA: A version of LDA allows each class to have its own covariance matrix. This produces a quadratic decision boundary.

**Decision Boundary**: The function that separates classes.

# Exploratory Data Analysis: Introductory Visualizations

## Density Plot of Age and Class of DIED

The code chunk below produces density plots. The area is of each density plot represents the distribution of age corresponding to DIED=0 (survied) or DIED=1 (died), while the color of the density plot's area represents the class (DIED=0 or DIED=1).
```{r Density Plot of Age and Class of DIED}
# Density Plot 
ggplot(data, aes(x = AGE, fill = DIED)) +
  geom_density(alpha = 0.5) +
  labs(title = 'Age Distribution by Class of DIED', x='Age', y='Density') 

# Get the means of the ages of died and didn't die
died_age <- data %>% filter(DIED==1)
surv_age <- data %>% filter(DIED==0)
mean(died_age$AGE)
mean(surv_age$AGE)
```
Both distributions have a relatively bell shaped curve, suggesting normality. The age of patients who died has a mean around 62.44, which is greater than the age of patients who did not die, whose distribution had a mean of 48.5.

## Distributions of Binary Predictors

In the graphs below, the distributions of pre-existing conditions are often associate with death by COVID-19 are shown.

```{r Bar Plots: Intubed}
# Create function to use for each variable
bar <- function(data, var) {
  ggplot(data = data, aes_string(x = var)) +
    geom_bar() +
    labs(title = paste("Distribution of", var), x = var, y = "Count"
    )
}
```

```{r Bar Plots: PNEUMONIA}
# Bar plot for PNEUMONIA
bar(data, 'PNEUMONIA')
```
There were more patients with with pneumonia than without pneumonia.

```{r Bar Plots: DIABETES}
# Bar plot for DIABETES
bar(data, 'DIABETES')
```
Significantly More patients did not have diabetes than did.

```{r Bar Plots: OTHER_DISEASE}
# Bar plot for HIPERTENSION
bar(data, 'HIPERTENSION')
```
The  majority of patients did not have hypertension

```{r Bar Plots: CARDIOVASCULAR}
# Bar plot for CARDIOVASCULAR
bar(data, 'CARDIOVASCULAR')
```
The vast majority of patients did not have cardiovascular disease

```{r Bar Plots: OBESITY}
# Bar plot for OBESITY
bar(data, 'OBESITY')
```
The majority of patients were not obese.

# Exploratory Data Analysis: Assumptions

**The Primary Assumptions of LDA and QDA are:**

1. The outcome variable must be categorical; our outcome variable, **DIED**, is categorical. This was ensured in the code above.

2. LDA and QDA perform optimally when the predictor variables are continuous and normally distributed.

3. LDA and QDA assume that the variance is constant among classes in the outcome variable (homoscedasticity).

4. LDA and QDA assume that the variables are independent.

## Assumption 2: Continuous Predictor variables are Normally Distributed
```{r Normality of Age: Visual Assessment}
# Normality of continuous variables
ggqqplot(data$AGE)
```
The one continuous predictor variable, age, may be normally distributed, but it is unclear. A Shapiro-Test is necessary.

```{r Normality of Age: Numerical Assessment}
# Normality of continuous variables
shapiro.test(sample(data$AGE,size=5000))
```

The output of the Shapiro-Wilk test provides evidence that age is not normally distributed. As age is the only continuous variable, it is unlikely that its distribution will greatly affect the model.

## Assumption 3: The variance is constant among classes in the outcome variable (homoscedasticity)

```{r Assumption: Constant variance of DIED}
# Normality of continuous variables of DIED
leveneTest(AGE ~ DIED, data = data)
```
The assumption of homoscedasticity, which is the most important assumption in LDA and QDA analysis, is not violated.

## Assumption 4: Sample measurements are independent from each-other

The design of the experiment suggests that the variables are independent.

# Variable Selection

In this section, the variables that are most closely associated with COVID-19 death are defined. We must eliminate variables with any multi-collinearity or near-zero-variance first.

```{r Finding Variables With High Collinearity}
# Calculate VIF (from car package)
multi_col_model <- lm(DIED ~ ., data = data)
vif(multi_col_model)
```

```{r Finding variables with near zero variance}
# Calculate the near-zero-variance (from caret)
near_zero_var <- nearZeroVar(data, saveMetrics = TRUE)
near_zero_var <- rownames(near_zero_var[near_zero_var$nzv == TRUE,])
near_zero_var
```

The variables found to have multi-collinearity, near-zero-variance, or other negative traits are removed in the chunk below.

```{r Removing variables with High Collinearity and Near Zero Variance}
data <- data[,!names(data) %in% c("MEDICAL_UNIT", "USMER", "CLASIFFICATION_FINAL", "PREGNANT", "COPD", "ASTHMA", "IMNSUPR", "TOBACCO")]
str(data)
```

# Variable Selection

One of the most important parts of the model building process is variable selection. In this section, best subsets variable selection was used. As best subsets selection does not work on classes of 'lda' or 'qda', a logistic regression is used.

```{r Variable Selection}
# LDA variable selection
full_model <- glm(DIED ~ ., data = data, family = binomial, trace = TRUE)
step_model <- stepAIC(full_model, direction = "both")
summary(step_model)
```
According to the step-wise selection, all variables are highly statistically significant predictors of death by COVID-19. This is likely due to to previous heavy pruning of variables earlier with tests such as tests for multi-collinearity and non-zero-variance. 

As the model's deviance did not drop by removing variables, as shown by the output of the stepAIC function, it is not necessary to remove any variables. Furthermore, these variables have already been tested for multi-collinearity.

# Fitting the LDA Model

In the code below, the LDA Model is fit on the variables output by stepAIC.

```{r LDA}
# Set the seed
set.seed(1)

# Fit the LDA model
lda_model <- lda(DIED ~ ., data = data)

# Make predictions
lda_predictions <- predict(lda_model)

# Confusion matrix
lda_conf <- table(Predicted = lda_predictions$class, Actual = data$DIED)
lda_conf
```


# Fitting the QDA Model

In the code below, the QDA Model is fit on the variables output by stepAIC.

```{r QDA}
# Set the seed
set.seed(1)

# Fit the LDA model
qda_model <- qda(DIED ~ ., data = data)

# Make predictions
qda_predictions <- predict(qda_model)

# Confusion matrix
qda_conf <- table(Predicted = qda_predictions$class, Actual = data$DIED)
qda_conf
```
These models confirm that lda and qda work with the data. In the section below, they are refit on testing data, and their accuracy is tested.

# Splitting the Data into Training Data and Testing Data

In order to test the accuracy, among other metrics, of the model, the train-test paradigm will be used.

```{r Train Test Split}
set.seed(1)

# Establish n
n <- nrow(data)

# Train test split
tts <- rep(0:1,c(round(n*.3), n-round(n*.3)))

# Get the TTS Split
tts.split <- sample(tts, n)

# Visualize the split (0 = Testing, 1 = Training)
table(tts.split)
```

In the table above, the train test split is established. Approximately 70% (53782) of the data is allocated as testing data, while the other 30% (23050) is allocated as training data. 

In the code-chunk below, the training and testing data is established
```{r Establish Training and Testing Data}
# Establish training and testing data
training_data <- data[tts.split==1, ]
testing_data <- data[tts.split==0, ]
```

# Refitting the Model with the Train-Test Paradigm and Testing Model Accuracy

In the code chunk below, the models are fit on the training and testing data.
```{r Fitting LDA and QDA with Training Data}
# Fit the LDA and QDA model on the training data
lda_model <- lda(DIED ~ ., data = training_data)
qda_model <- qda(DIED ~ ., data = training_data)

cat('\n','LDA MODEL', '\n')
lda_model
cat('\n','QDA MODEL', '\n')
qda_model
```

Then the training models are used to make predictions on the testing data.

```{r LDA and QDA Predictions}
# Make predictions on testing data
lda_model_predictions <- predict(lda_model, testing_data)$class
qda_model_predictions <- predict(qda_model, testing_data)$class
```

The confusion matrix for each model is visualized below

```{r LDA and QDA Confusion Matrices}
# Visualize the tables
print("LDA MODEL")
lda_conf <- table(predicted_deaths = lda_model_predictions, actual_deaths = testing_data$DIED)
lda_conf
cat("\n")

print("QDA MODEL")
qda_conf <- table(predicted_deaths = qda_model_predictions, actual_deaths = testing_data$DIED)
qda_conf
cat("\n")
```

# Chosing Best Model

## Determining Model Accuracy
In the code below, the best model is chosen based on its accuracy, recall, and precision.

```{r Assessing Accuracy}
# Get metrics for LDA
lda_tn <- lda_conf[1,1]
lda_fn <- lda_conf[1,2]
lda_fp <- lda_conf[2,1]
lda_tp <- lda_conf[2,2]

# Get metrics for QDA
qda_tn <- qda_conf[1,1]
qda_fn <- qda_conf[1,2]
qda_fp <- qda_conf[2,1]
qda_tp <- qda_conf[2,2]

# Get accuracy for LDA
lda_accuracy <- sum(lda_tp + lda_tn) / sum(lda_tn + lda_fn + lda_fp + lda_tp)

# Get accuracy for QDA
qda_accuracy <- sum(qda_tp + qda_tn) / sum(qda_tn + qda_fn + qda_fp + qda_tp)

# Get recall for LDA
lda_recall <- lda_tp / sum(lda_tp, lda_fp)

# Get recall for QDA
qda_recall <- qda_tp / sum(qda_tp, qda_fp)

# Get precision for LDA
lda_precision <- lda_tp / sum(lda_tp, lda_fp)

# Get precision for QDA 
qda_precision <- qda_tp / sum(qda_tp , lda_fp)

# Print
cat('LDA ACCURACY: ', lda_accuracy, '\n')
cat('QDA ACCURACY: ', qda_accuracy, '\n')
cat('\n')
cat('LDA RECALL: ', lda_recall, '\n')
cat('QDA RECALL: ', qda_recall, '\n')
cat('\n')
cat('LDA PRECISION: ', lda_precision, '\n')
cat('QDA PRECISION: ', qda_precision, '\n')
cat('\n')


```
As the LDA model has higher accuracy and higher recall, it is chosen as the best model, and will be assessed moving forward.

The LDA model is 77% accurate in making classifications, meaning that 77% of all predictions made by the model will be correct.

# Understanding the Best Model

The coefficients of the best model are output below.
```{r Understanding The Best Model}
# Printing the coefficients of the model
lda_model$scaling
```
The coefficients printed above are linear discriminant coefficients. Each linear discriminant coefficient indicates how much the variable contributes to the observation's class determination, 'DIED' = 0 or 'DIED' = 1.

The coefficients indicate that variables are positively or negatively associated with death from COVID-19.

**Positive Association With Death by COVID-19**
- AGE
- CARDIOVASCULAR
- ICU

**Negative Association With Death by COVID-19**
- INTUBED
- PNEUMONIA
- DIABETES
- INMSUPR
- HIPERTENSION
- OTHER_DISEASE
- OBESITY
- RENAL_CHRONIC

# The 'Cut Off' Parameter and Measures of Accuracy

## LDA/QDA 'Cut Off' Parameter

The **Cut Off* parameter in LDA and QDA models determines the point (probability) at which an observation is considered one class of the outcome variable or another. The Cut Off parameter, by default, is set to 0.5.

For instance, in this example, the Cut Off parameter determines whether or not an observation, output by the classification model, is classified as 'DIED' == 1 (died) or 'DIED' == 0 (survived). If the Cut Off parameter is set to 0.5, all observations resulting in a probability of 50% and higher of death, as predicted by the LDA or QDA model, would have their outcome variable's class, 'DIED', set to 1.

### Affect of Cut Off On Recall

Increasing the Cut Off parameter forces less parameters to be considered positive. In this scenario, increasing the Cut Off parameter would cause less deaths to be predicted.

Therefore, more true positives may be considered negative. This causes an increased number of false negatives, reducing the recall.

### Affect of Cut Off On Precision

Alternatively, increasing the Cut Off parameter can increase precision by reducing the number of false positives.

# ROC Curve and Conclusion

```{r ROC Curve}
# Cut Off
cutoff <- 0.5

# Fix lda_model_predictions variable
lda_model_predictions <- predict(lda_model, testing_data)

# ROC Curve Step 1: Get probabilities for positive class
lda_probabilities <- lda_model_predictions$posterior[,2]

# ROC Curve Step 2: Create ROC curve
lda_roc <- roc(testing_data$DIED, lda_probabilities)

# ROC Curve Step 3: Create dataframe with columns tpr and fpr
lda_roc_df <- data.frame(tpr = lda_roc$sensitivities, fpr = 1 - lda_roc$specificities)

# ROC Curve Step 4: Graph
ggplot(lda_roc_df, aes(x = fpr, y = tpr)) +
  geom_line(color = 'blue') +
  labs(title = "ROC Curve for LDA Model",
       x = "False Positive Rate",
       y = "True Positive Rate") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed")
```
The blue ROC Curve represents the True Positive Rate against the False Positive Rate; it is compared to the dashed line, which represents typical random chance (an AUC of 0.5). 

The greater the area under the curve (the closer the function gets to the top left corner of the graph), the more accurate the model is.

The area under the curve is calculated below.

```{r AUC: Area Under Curve}
# Area under curve computation
auc(lda_roc)
```

The Area Under the Curve Calculation is 0.8, meaning that the model performs 30% (calculation: 0.80-0.50) better than random chance. 

In the case of this analysis, it is more concerning to make a false negative than a false positive, as the outcome of a false negative may be death without treatment. If a false positive is predicted, the patient can take actions to prevent death by COVID-19; if a flase negative is produced, a patient cannot.

# Conclusions

By convention, an AUC score from 0.8-0.9 is considered excellent, meaning that the model's prediction of death by COVID-19 is 'excellent'. The accuracy of the model is 77%, meaning that 77% of all predictions will be correct.
